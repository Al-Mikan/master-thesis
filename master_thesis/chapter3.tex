\chapter{提案手法}
本研究では，ラベル付けされたデータが乏しい環境下や，
学習時に含まれない未知の動物種に対しても頑健に機能する，
行動に基づく特徴表現空間を構築することを目的とする．
提案手法は，
RGB 映像から得られる外見情報と，
オプティカルフローから得られる動作情報を
適応的に統合するマルチモーダル学習フレームワークである．
さらに，学習された特徴空間から
動物種に依存するバイアスを除去するため，
敵対的学習に基づく種情報分離モジュールを導入する．

システム全体のアーキテクチャを図\ref{fig:architecture}に示す．
入力映像はスライディングウィンドウによって複数のクリップに分割され，
それぞれ RGB ストリームおよびオプティカルフローストリームによって
特徴抽出が行われる．
抽出された特徴は Gated Fusion により統合された後，
距離学習によって行動クラスごとのクラスタリングが促進される．
同時に，敵対的種識別器を用いることで，
埋め込み空間から動物種に由来する情報の除去が行われる．


\section{入力動画の前処理}
入力映像は，撮影時間やフレーム数が動画ごとに異なるため，
可変長の映像をそのまま処理することは困難である．
そこで本研究では，
映像を一定長のクリップ列に分割するために
Sliding Window モジュールを採用する．

具体的には，
特徴抽出エンコーダの入力サイズに合わせて，
映像を固定フレーム長のクリップへと分割する．
例えば，エンコーダが 16 フレームを入力とする場合，
フレーム 1--16，2--17，3--18 のように，
1 フレームずつ重複させながらクリップを生成する．
このような重なりを持つ分割により，
映像中の局所的な動作の連続性を保持したまま
特徴抽出を行うことが可能となる．

各クリップは独立にエンコーダへ入力され，
クリップ単位の特徴ベクトルが得られる．
その後，得られた複数の特徴ベクトルは，
時間方向に沿って集約され，
Adaptive Average Pooling により
映像全体を表現する単一の特徴ベクトルへと変換される．
この処理により，
映像長に依存しない一定次元の埋め込み表現を
安定して得ることができる．

Sliding Window を用いた特徴抽出は，
フレームを等間隔にサンプリングする手法と比較して，
短時間に発生する動作変化や局所的な運動パターンを
より詳細に捉えることが可能であり，
行動認識において有効である．

\section{RGB 映像に基づく外観特徴抽出}
RGB ストリームは，
映像フレーム列から動物の外観や姿勢，
および周囲環境の文脈情報を捉えることを目的とする．
本研究では，
事前学習済みの VideoMAE エンコーダを用いて，
各クリップから高次元の外観特徴を抽出する．

外見情報の抽出には，
事前学習済みの VideoMAE を用いる．
VideoMAE は映像内の文脈的特徴や
大域的な外見情報を捉える能力に優れており，
本研究ではエンコーダ部分の重みを凍結した状態で使用する．
これにより，
動物の姿勢や周囲環境の文脈情報を
768 次元の特徴ベクトルとして抽出する．

\section{オプティカルフローに基づく運動特徴抽出}


オプティカルフローストリームは，
動物の動作に起因する時間的変化を捉えるために設計されている．
まず，
隣接フレーム間の画素移動量を推定するため，
RAFT に基づくオプティカルフロー推定を行う．

得られたオプティカルフローは，
X3D エンコーダへ入力され，
微細な運動パターンを表現する特徴ベクトルへと変換される．
オプティカルフローは，
被写体の色や模様に依存せず，
動きそのものを表現できるため，
外観差の大きい動物種間においても
共通性の高い特徴を抽出できる利点を持つ．


\section{Gated Fusion による適応的特徴統合}

外見情報と動作情報は互いに補完的であるが，
行動の種類や状況によって
重要度は大きく異なる．
そこで本研究では，
両者を単純に連結するのではなく，
Gated Fusion モジュールを用いて
適応的な重み付けを行う．

まず，
X3D からの特徴量 $\mathbf{E}_{X3D} \in \mathbb{R}^{2048}$ と
VideoMAE からの特徴量
$\mathbf{E}_{VideoMAE} \in \mathbb{R}^{768}$ を，
線形層を用いて共通の
512 次元空間へ射影する．

次に，以下の式により
ゲーティングベクトル
$\boldsymbol{\alpha} \in [0,1]^{512}$ を算出する．

\begin{equation}
  \boldsymbol{\alpha}
  = \sigma(\mathbf{W}[\mathbf{E}_{X3D}; \mathbf{E}_{VideoMAE}] + \mathbf{b})
  \label{eq:gating_vector}
\end{equation}

ここで，
$[\cdot;\cdot]$ はベクトルの連結，
$\mathbf{W} \in \mathbb{R}^{512 \times 1024}$ および
$\mathbf{b} \in \mathbb{R}^{512}$ は学習可能なパラメータ，
$\sigma(\cdot)$ はシグモイド関数を表す．

最終的な統合特徴量 $\mathbf{F}$ は，
次式により計算される．

\begin{equation}
  \mathbf{F}
  = \mathbf{E}_{X3D} \odot \boldsymbol{\alpha}
  + \mathbf{E}_{VideoMAE} \odot (1 - \boldsymbol{\alpha})
  \label{eq:fused_feature}
\end{equation}

ここで，
$\odot$ は要素ごとの積を示す．
この機構により，
モデルは行動や環境条件に応じて，
外見情報と動作情報の寄与度を柔軟に調整できる．

\section{敵対的学習による種情報の分離}

異なる動物種間での汎化性能を高めるためには，
埋め込み空間から種に依存した情報を排除し，
純粋な行動情報のみを保持する必要がある．
そこで本研究では，
Domain-Adversarial Neural Networks（DANN）に基づく
敵対的学習を導入する．

統合特徴量 $\mathbf{F}$ に対して
種識別器 $D_s$ を接続し，
その前段に勾配反転層
（Gradient Reversal Layer: GRL）を配置する．
学習過程において，
種識別器は種ラベル $y_s$ を正しく分類するよう学習する一方，
特徴抽出器および統合層は，
GRL により反転された勾配を受け取ることで，
種識別を困難にする特徴表現を生成するよう更新される．

敵対的損失 $L_{adv}$ は，
以下の交差エントロピー誤差として定義される．

\begin{equation}
  L_{adv}
  = -\frac{1}{N}
  \sum_{i=1}^{N}
  \sum_{k=1}^{K}
  y_{s,i}^{(k)}
  \log D_s(\text{GRL}(f(x_i)))^{(k)}
  \label{eq:adv_loss}
\end{equation}

ここで，
$N$ はバッチサイズ，
$K$ は種クラス数，
$f(x_i)$ は入力映像 $x_i$ に対する
特徴抽出および統合処理を表す．
距離学習によって行動判別性が維持されるため，
種情報のみを抑制しつつ，
行動識別能力を損なわない表現学習が可能となる．

\section{目的関数と最適化}

提案手法全体の目的関数 $L_{total}$ は，
距離学習による損失 $L_{triplet}$ と
敵対的損失 $L_{adv}$ の加重和として定義される．

\begin{equation}
  L_{total} = L_{triplet} + \lambda L_{adv}
  \label{eq:total_loss}
\end{equation}

ここで，
$L_{triplet}$ は
同一行動のサンプルを近づけ，
異なる行動のサンプルを遠ざける
トリプレット損失であり，
行動識別的なクラスタリングを実現するために不可欠である．

ハイパーパラメータである
トリプレット損失のマージン $m$，
および敵対的損失の重み係数 $\lambda$ は，
検証データセットにおける
ARI（Adjusted Rand Index）を最大化するように最適化を行った．
探索範囲は
$m \in [0.05, 0.5]$，
$\lambda \in [0.01, 1.0]$ と設定した．

